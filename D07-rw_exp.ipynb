{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ea5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4875269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# Set all the seeds for reproducibility\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a849c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffa79f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epastor/projects/divexplorer-journal/divexplorer-general/utils_data.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"length_of_stay\"] = pd.to_timedelta(\n"
     ]
    }
   ],
   "source": [
    "from utils_data import load_dataset_undiscretized\n",
    "\n",
    "\n",
    "dataset_name = 'compas'\n",
    "\n",
    "df, attributes, target_col_name, type_outcome = load_dataset_undiscretized(dataset_name)\n",
    "\n",
    "if dataset_name == 'bike':\n",
    "    cat_attr_list = ['season', 'holiday',  'weathersit', 'workingday'] \n",
    "    numeric_feature_cols = ['temp', 'atemp', 'hum', 'windspeed', 'hr', 'weekday',  'mnth', 'yr']\n",
    "elif dataset_name == 'airbnb':\n",
    "    df[attributes] = df[attributes].astype('str')\n",
    "    df_filled = df.fillna('nan')\n",
    "    cat_attr_list = attributes\n",
    "    numeric_feature_cols = []\n",
    "else:\n",
    "    cat_attr_list = [col for col in attributes if df[col].dtype == \"object\"]\n",
    "    numeric_feature_cols = [col for col in attributes if df[col].dtype != \"object\"]\n",
    "\n",
    "# FORCE CONVERSION  \n",
    "if type_outcome == 'quantitative':\n",
    "    from utils import discretize_column\n",
    "    df['class'] = discretize_column(\n",
    "        df, target_col_name, bins=2\n",
    "    )\n",
    "    df['class'] = pd.factorize(df['class'])[0]\n",
    "    df.drop(columns=[target_col_name])\n",
    "    type_outcome = 'boolean'\n",
    "    target_col_name = 'class'\n",
    "\n",
    "# if dataset_name == 'bike':\n",
    "#     # Divide the dataset into training and testing sets - continuous splitting\n",
    "#     index_end_train = int(len(df)*0.7)\n",
    "#     df_train, df_test = df[attributes].loc[0:index_end_train], df[attributes].loc[index_end_train+1:]\n",
    "# else:\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "df_train, df_test = model_selection.train_test_split(df[attributes], train_size=0.70)\n",
    "\n",
    "y_train = df.loc[df_train.index, target_col_name]\n",
    "y_test = df.loc[df_test.index, target_col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1be4c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.658207343412527\n"
     ]
    }
   ],
   "source": [
    "from utils_data import encode_data\n",
    "\n",
    "# We discretize\n",
    "\n",
    "from utils import discretize\n",
    "df_discretized = discretize(df[attributes], attributes=numeric_feature_cols, bins=3, round_v=0, dataset_name=dataset_name)\n",
    "if dataset_name == 'bank':\n",
    "    # Problem in converting pdays and previous\n",
    "    df_discretized['pdays'] = df_discretized['pdays'].apply(lambda x: '-1' if x > 0 else '>=0')\n",
    "    import numpy as np\n",
    "\n",
    "    conditions = [\n",
    "        df_discretized['previous'] == 0,\n",
    "        df_discretized['previous'].between(1, 3, inclusive=\"both\"),\n",
    "        df_discretized['previous'] > 3\n",
    "    ]\n",
    "\n",
    "    choices = ['0', '[1-3]', '>3']\n",
    "\n",
    "    df_discretized['previous'] = np.select(conditions, choices)\n",
    "    \n",
    "df_discretized_train, df_discretized_test = df_discretized.loc[df_train.index], df_discretized.loc[df_test.index]\n",
    "\n",
    "discretize_first = True\n",
    "if discretize_first: #Relevant for SliceFinder\n",
    "    # We encode the data\n",
    "    df_train_enc, df_test_enc, cat_encoders, scalers = encode_data(df_discretized_train, df_discretized_test , attributes)\n",
    "\n",
    "else:\n",
    "\n",
    "    df_train_enc, df_test_enc, cat_encoders, scalers = encode_data(df_train, df_test, cat_attr_list)\n",
    "\n",
    "\n",
    "if type_outcome == 'boolean':\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    model.fit(df_train_enc, y_train);\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    y_test_pred = model.predict(df_test_enc)\n",
    "\n",
    "    training_score = accuracy_score(y_test, y_test_pred)\n",
    "else:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    model.fit(df_train_enc, y_train);\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    y_test_pred = model.predict(df_test_enc)\n",
    "    training_score = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(training_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70ea3d",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c39d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59092d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discretized_analyze = df_discretized_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9214c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support_count = min_support*len(df_discretized_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88b03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_series = pd.Series(y_test_pred, index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90021c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def run_with_timeout(func, timeout, *args, **kwargs):\n",
    "    queue = multiprocessing.Queue()\n",
    "    p = multiprocessing.Process(target=func, args=(queue, *args), kwargs=kwargs)\n",
    "    p.start()\n",
    "    p.join(timeout)  # wait at most timeout seconds\n",
    "    if p.is_alive():\n",
    "        print(f\"{func.__name__} took more than {timeout} seconds, terminating.\")\n",
    "        p.terminate()  # kills the process and all child processes\n",
    "        p.join()\n",
    "        return None\n",
    "    else:\n",
    "        result = queue.get()\n",
    "        if isinstance(result, Exception):\n",
    "            raise result\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de5738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout_limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5482c91",
   "metadata": {},
   "source": [
    "# DivExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a89a813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0536653995513916 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemset</th>\n",
       "      <th>acc</th>\n",
       "      <th>acc_div</th>\n",
       "      <th>acc_t</th>\n",
       "      <th>length</th>\n",
       "      <th>support_count</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.100432</td>\n",
       "      <td>(sex=Male, c_charge_degree=F, age_cat=25 - 45, priors_count=[1,3])</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>-0.168960</td>\n",
       "      <td>4.439646</td>\n",
       "      <td>4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.510753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.123110</td>\n",
       "      <td>(c_charge_degree=F, age_cat=25 - 45, priors_count=[1,3])</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>-0.145049</td>\n",
       "      <td>4.180681</td>\n",
       "      <td>3</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.486842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.125810</td>\n",
       "      <td>(race=Caucasian, priors_count=[1,3])</td>\n",
       "      <td>0.540773</td>\n",
       "      <td>-0.117435</td>\n",
       "      <td>3.432963</td>\n",
       "      <td>2</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0.459227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      support  \\\n",
       "157  0.100432   \n",
       "121  0.123110   \n",
       "119  0.125810   \n",
       "\n",
       "                                                                itemset  \\\n",
       "157  (sex=Male, c_charge_degree=F, age_cat=25 - 45, priors_count=[1,3])   \n",
       "121            (c_charge_degree=F, age_cat=25 - 45, priors_count=[1,3])   \n",
       "119                                (race=Caucasian, priors_count=[1,3])   \n",
       "\n",
       "          acc   acc_div     acc_t  length  support_count     error  \n",
       "157  0.489247 -0.168960  4.439646       4          186.0  0.510753  \n",
       "121  0.513158 -0.145049  4.180681       3          228.0  0.486842  \n",
       "119  0.540773 -0.117435  3.432963       2          233.0  0.459227  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from divexplorer import DivergenceExplorer\n",
    "\n",
    "df_discretized_analyze_de = df_discretized_test.copy()\n",
    "\n",
    "if dataset_name == \"bike\":\n",
    "    target_col = \"mae\"\n",
    "    type_outcome = \"quantitative\"\n",
    "\n",
    "    df_discretized_analyze_de[target_col] = abs(y_test - y_test_pred)\n",
    "\n",
    "else:\n",
    "    target_col = \"acc\"\n",
    "    from divexplorer.outcomes import get_accuracy_outcome\n",
    "\n",
    "    df_discretized_analyze_de[target_col] = get_accuracy_outcome(y_test, y_test_pred)\n",
    "\n",
    "import time\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_discretized_analyze_de)\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    if type_outcome == \"boolean\":\n",
    "        subgroups = fp_diver.get_pattern_divergence(\n",
    "            min_support=min_support,\n",
    "            boolean_outcomes=[target_col],\n",
    "            FPM_algorithm=\"fpgrowth\",\n",
    "        )\n",
    "    else:\n",
    "        subgroups = fp_diver.get_pattern_divergence(\n",
    "            min_support=min_support,\n",
    "            quantitative_outcomes=[target_col],\n",
    "            FPM_algorithm=\"fpgrowth\",\n",
    "        )\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % end_time)\n",
    "except MemoryError:\n",
    "    print(\"Memory limit exceeded!\")\n",
    "if type_outcome == \"boolean\":\n",
    "    subgroups[\"error\"] = 1 - subgroups[\"acc\"]\n",
    "    max_row = subgroups.loc[subgroups[\"error\"].idxmax()]\n",
    "\n",
    "    max_de = max_row[\"error\"]\n",
    "    len_of_max_de = max_row[\"length\"]\n",
    "else:\n",
    "    max_row = subgroups.loc[subgroups[target_col].idxmax()]\n",
    "    max_de = max_row[target_col]\n",
    "    len_of_max_de = max_row[\"length\"]\n",
    "results[\"divexplorer\"] = {\n",
    "    \"time\": end_time,\n",
    "    \"number_subgroups\": len(subgroups),\n",
    "    \"max_s\": max_de,\n",
    "    \"len_of_max\": len_of_max_de,\n",
    "    \"max_len\": subgroups[\"length\"].max(),\n",
    "}\n",
    "subgroups.sort_values(target_col).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba07a3c",
   "metadata": {},
   "source": [
    "# Slice Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b138a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to adapt  run_slice_line for queue\n",
    "def run_slice_line_wrapper(queue, df_discretized_analyze_sl, test_errors, min_support_count, sl_params):\n",
    "    from utils_rw import run_slice_line\n",
    "    sl, end_time = run_slice_line(df_discretized_analyze_sl, test_errors, min_support_count, sl_params)\n",
    "    queue.put((sl, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c6577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epastor/miniconda3/envs/divexp/lib/python3.13/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.08916521072387695 seconds ---\n"
     ]
    }
   ],
   "source": [
    "## SLice line\n",
    "df_discretized_analyze_sl = df_discretized_test.copy()\n",
    "df_discretized_analyze_sl[attributes] = df_discretized_analyze_sl[attributes].astype(\n",
    "    str\n",
    ")\n",
    "\n",
    "test_errors = y_test != y_test_pred\n",
    "\n",
    "min_support_count = min_support * len(df_discretized_analyze_sl)\n",
    "\n",
    "sl_params = {\"alpha\": 0.95, \"k\": 100, \"max_l\": len(attributes)}\n",
    "\n",
    "# I want that if the method does not finish in timeout_limit seconds, it is killed\n",
    "sl, end_time  = run_with_timeout(\n",
    "    run_slice_line_wrapper,\n",
    "    timeout_limit,\n",
    "    df_discretized_analyze_sl,\n",
    "    test_errors,\n",
    "    min_support_count,\n",
    "    sl_params)\n",
    "    \n",
    "if sl is not None:\n",
    "\n",
    "    from utils_rw import encode_slices_df_with_metric_sl\n",
    "\n",
    "    sl_subgroups_df = encode_slices_df_with_metric_sl(\n",
    "        sl, attributes, df_discretized_analyze_sl, y_test, y_test_pred_series\n",
    "    )\n",
    "\n",
    "    # We only use the error metric for comparison\n",
    "    metric_name = \"error\"\n",
    "\n",
    "    if len(sl_subgroups_df) > 0:\n",
    "        max_row = sl_subgroups_df.loc[sl_subgroups_df[metric_name].idxmax()]\n",
    "        max_sl = max_row[metric_name]\n",
    "        len_of_max_sl = max_row[\"length\"]\n",
    "        max_len = sl_subgroups_df[\"length\"].max()\n",
    "\n",
    "    else:\n",
    "        max_row, max_sl, len_of_max_sl = 0, 0, 0\n",
    "\n",
    "\n",
    "results[\"sliceline\"] = {\n",
    "    \"time\": end_time,\n",
    "    \"number_subgroups\": len(sl_subgroups_df),\n",
    "    \"slf_params\": sl_params,\n",
    "    \"max_s\": max_sl,\n",
    "    \"len_of_max\": len_of_max_sl,\n",
    "    \"max_len\": max_len,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300249c",
   "metadata": {},
   "source": [
    "# Slice Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fd7a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def run_slice_finder_wrapper(queue, *args, **kwargs):\n",
    "    from utils_rw import run_slice_finder\n",
    "    \"\"\"Run SliceFinder and put result in a queue.\"\"\"\n",
    "    try:\n",
    "        result = run_slice_finder(*args, **kwargs)\n",
    "        queue.put(result)\n",
    "    except Exception as e:\n",
    "        queue.put(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree 1\n",
      "crossing\n",
      "effect size filtering\n",
      "degree 2\n",
      "crossing\n",
      "effect size filtering\n",
      "degree 3\n",
      "crossing\n",
      "effect size filtering\n",
      "sorting\n",
      "--- 204.56702995300293 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_discretized_analyze_sf = df_discretized_analyze.copy()\n",
    "df_discretized_analyze_sf[attributes] = df_discretized_analyze_sf[attributes].astype(str)\n",
    "\n",
    "timeout_limit = 3600\n",
    "\n",
    "sf_params = {\"degree\": 3, \"k\": 100}\n",
    "\n",
    "from utils_rw import encode_slices_df_with_metric_sf\n",
    "\n",
    "recommendations_end_time = run_with_timeout(\n",
    "    run_slice_finder_wrapper,\n",
    "    timeout_limit,\n",
    "    df_test_enc, y_test, model,\n",
    "    degree=sf_params[\"degree\"], k=sf_params[\"k\"]\n",
    ")\n",
    "\n",
    "if recommendations_end_time is not None:\n",
    "    recommendations, end_time_sf = recommendations_end_time\n",
    "\n",
    "    # df_slices = encode_slices_df(recommendations, cat_encoders)\n",
    "\n",
    "    y_test_pred_series = pd.Series(y_test_pred, index=df_test.index)\n",
    "\n",
    "    sf_slices_df = encode_slices_df_with_metric_sf(\n",
    "        recommendations, cat_encoders, df_discretized_analyze_sf, y_test, y_test_pred_series\n",
    "    )\n",
    "\n",
    "    metric_name = \"error\"\n",
    "\n",
    "    max_overall, len_of_max_overall_sf, max_sf, len_of_max_sf = 0, 0, 0, 0\n",
    "\n",
    "    if len(sf_slices_df) > 0:\n",
    "\n",
    "        max_row_overall = sf_slices_df.loc[sf_slices_df[metric_name].idxmax()]\n",
    "\n",
    "        max_overall = max_row_overall[metric_name]\n",
    "        len_of_max_overall_sf = max_row_overall[\"length\"]\n",
    "\n",
    "        filtered_above_support = sf_slices_df.loc[sf_slices_df[\"size\"] >= min_support_count]\n",
    "\n",
    "        if len(filtered_above_support) > 0:\n",
    "            max_row_above_supp = filtered_above_support.loc[\n",
    "                filtered_above_support[metric_name].idxmax()\n",
    "            ]\n",
    "\n",
    "            max_sf = max_row_above_supp[metric_name]\n",
    "            len_of_max_sf = max_row_above_supp[\"length\"]\n",
    "\n",
    "    results[\"slicefinder\"] = {\n",
    "        \"time\": end_time_sf,\n",
    "        \"number_subgroups\": len(sf_slices_df),\n",
    "        \"sf_params\": sf_params,\n",
    "        \"max_s\": max_sf,\n",
    "        \"max_overall\": max_overall,\n",
    "        \"len_of_max\": len_of_max_sf,\n",
    "        \"max_len\": max_sf,\n",
    "        \"len_of_max_overall_sf\": len_of_max_overall_sf,\n",
    "    }\n",
    "else:\n",
    "    results[\"slicefinder\"] = {\n",
    "        \"time\": f\"> {timeout_limit} sec\",\n",
    "        \"number_subgroups\": 0,\n",
    "        \"sf_params\": sf_params,\n",
    "        \"max_s\": \"timeout\",\n",
    "        \"max_overall\": \"timeout\",\n",
    "        \"len_of_max\": 0,\n",
    "        \"max_len\": 0,\n",
    "        \"len_of_max_overall_sf\": 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the result in a tabular format\n",
    "# \n",
    "from copy import deepcopy\n",
    "\n",
    "res_dataset = deepcopy(results)\n",
    "\n",
    "results = {}\n",
    "\n",
    "results[dataset_name] = { 'divexplorer': res_dataset['divexplorer'], 'sliceline': res_dataset['sliceline'], 'slicefinder': res_dataset['slicefinder']}\n",
    "\n",
    "output = []\n",
    "\n",
    "for dataset_name in results:\n",
    "    for method in results[dataset_name]:\n",
    "        input_dict = results[dataset_name][method]\n",
    "        if 'max_overall' in input_dict:\n",
    "            max_overall = input_dict['max_overall']\n",
    "            len_of_max_overall = input_dict['len_of_max_overall_sf']\n",
    "        else:\n",
    "            max_overall = input_dict['max_s']\n",
    "            len_of_max_overall = input_dict['len_of_max']\n",
    "        output.append([dataset_name, method, input_dict['time'], input_dict['number_subgroups'], input_dict['max_s'], max_overall, input_dict['len_of_max'], len_of_max_overall])\n",
    "pd.DataFrame(output, columns = ['dataset', 'method', 'time', '#subgroups', 'max_s', 'max_overall', 'len_of_max', len_of_max_overall]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65efc0",
   "metadata": {},
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "results = {}\n",
    "output_dir = './results_rw/0.1/'\n",
    "\n",
    "for dataset_name in ['adult', 'bank', 'compas', 'compasoriginal', 'folktables', 'german', 'heart', 'law']:\n",
    "    filename = os.path.join(output_dir, f\"dict_rw_results_{dataset_name}.pkl\")\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "        print(f'{dataset_name} loaded')\n",
    "    results[dataset_name] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e75cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for dataset_name in results:\n",
    "    for method in results[dataset_name]:\n",
    "        input_dict = results[dataset_name][method]\n",
    "        if 'max_overall' in input_dict:\n",
    "            max_overall = input_dict['max_overall']\n",
    "            len_of_max_overall = input_dict['len_of_max_overall_sf']\n",
    "        else:\n",
    "            max_overall = input_dict['max_s']\n",
    "            len_of_max_overall = input_dict['len_of_max']\n",
    "        output.append([dataset_name, method, input_dict['time'], input_dict['number_subgroups'], input_dict['max_s'], max_overall, input_dict['len_of_max'], len_of_max_overall])\n",
    "pd.DataFrame(output, columns = ['dataset', 'method', 'time', '#subgroups', 'max_s', 'max_overall', 'len_of_max', len_of_max_overall]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4bb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "divexp",
   "language": "python",
   "name": "divexp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
